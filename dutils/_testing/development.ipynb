{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T18:53:35.328397800Z",
     "start_time": "2023-06-18T18:53:26.456959Z"
    }
   },
   "outputs": [],
   "source": [
    "import dutils as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T18:54:02.351498900Z",
     "start_time": "2023-06-18T18:54:02.328317800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:75% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dutils as U\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "U.jupyter_ipython.adjust_screen_width()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T18:54:16.194207600Z",
     "start_time": "2023-06-18T18:54:15.424127Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received bad path: `C:/Users/JK/Desktop/test2.jpg`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m images \u001B[38;5;241m=\u001B[39m [U\u001B[38;5;241m.\u001B[39mimages\u001B[38;5;241m.\u001B[39mload_image(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/JK/Desktop/test2.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m9\u001B[39m)]\n",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m images \u001B[38;5;241m=\u001B[39m [\u001B[43mU\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_image\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:/Users/JK/Desktop/test2.jpg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m9\u001B[39m)]\n",
      "File \u001B[1;32m~\\OneDrive\\Programming\\Python\\Modules\\_dutils\\dutils\\images.py:364\u001B[0m, in \u001B[0;36mload_image\u001B[1;34m(path, load_type)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;66;03m# Simple Checks\u001B[39;00m\n\u001B[0;32m    363\u001B[0m _type_check\u001B[38;5;241m.\u001B[39massert_type(path, \u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m--> 364\u001B[0m \u001B[43m_input_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;66;03m# Check load_type\u001B[39;00m\n\u001B[0;32m    367\u001B[0m legal_load_types \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrey\u001B[39m\u001B[38;5;124m\"\u001B[39m: _cv2\u001B[38;5;241m.\u001B[39mIMREAD_GRAYSCALE,\n\u001B[0;32m    368\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgray\u001B[39m\u001B[38;5;124m\"\u001B[39m: _cv2\u001B[38;5;241m.\u001B[39mIMREAD_GRAYSCALE,\n\u001B[0;32m    369\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb\u001B[39m\u001B[38;5;124m\"\u001B[39m: _cv2\u001B[38;5;241m.\u001B[39mIMREAD_COLOR,\n\u001B[0;32m    370\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbgr\u001B[39m\u001B[38;5;124m\"\u001B[39m: _cv2\u001B[38;5;241m.\u001B[39mIMREAD_COLOR,\n\u001B[0;32m    371\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munchanged\u001B[39m\u001B[38;5;124m\"\u001B[39m: _cv2\u001B[38;5;241m.\u001B[39mIMREAD_UNCHANGED}\n",
      "File \u001B[1;32m~\\OneDrive\\Programming\\Python\\Modules\\_dutils\\dutils\\input_output.py:27\u001B[0m, in \u001B[0;36massert_path\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     25\u001B[0m _type_check\u001B[38;5;241m.\u001B[39massert_type(path, \u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path_exists(path):\n\u001B[1;32m---> 27\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived bad path: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Received bad path: `C:/Users/JK/Desktop/test2.jpg`"
     ]
    }
   ],
   "source": [
    "images = [U.images.load_image(\"C:/Users/JK/Desktop/test2.jpg\") for _ in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = U.experimental.get_test_image()\n",
    "h, w, _ = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,_ = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(465, 606, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>epoch_trained_relative</th>\n",
       "      <th>epochs_trained_total</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_valid</th>\n",
       "      <th>precision_class</th>\n",
       "      <th>precision_avg_micro</th>\n",
       "      <th>precision_avg_macro</th>\n",
       "      <th>recall_class</th>\n",
       "      <th>recall_avg_micro</th>\n",
       "      <th>recall_avg_macro</th>\n",
       "      <th>f1_class</th>\n",
       "      <th>f1_avg_micro</th>\n",
       "      <th>f1_avg_macro</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-11 21:37:54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850509</td>\n",
       "      <td>0.59481</td>\n",
       "      <td>[0.30952, 0.29167, 0.11765]</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23961</td>\n",
       "      <td>[0.33333, 0.17949, 0.18182]</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23155</td>\n",
       "      <td>[0.32098, 0.22223, 0.14286]</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22869</td>\n",
       "      <td>[[13, 12, 14], [16, 7, 16], [13, 5, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-11 21:37:54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700464</td>\n",
       "      <td>0.798062</td>\n",
       "      <td>[0.38235, 0.40625, 0.20588]</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33149</td>\n",
       "      <td>[0.30952, 0.38235, 0.29167]</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32785</td>\n",
       "      <td>[0.3421, 0.39394, 0.24138]</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32581</td>\n",
       "      <td>[[13, 12, 17], [11, 13, 10], [10, 7, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-11 21:37:54</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.263107</td>\n",
       "      <td>0.825322</td>\n",
       "      <td>[0.45161, 0.48276, 0.225]</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38646</td>\n",
       "      <td>[0.41176, 0.4, 0.29032]</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36736</td>\n",
       "      <td>[0.43077, 0.4375, 0.25352]</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37393</td>\n",
       "      <td>[[14, 7, 13], [3, 14, 18], [14, 8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-11 21:37:54</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847592</td>\n",
       "      <td>0.915717</td>\n",
       "      <td>[0.4375, 0.15385, 0.5]</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.36378</td>\n",
       "      <td>[0.38889, 0.17391, 0.5122]</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.35833</td>\n",
       "      <td>[0.41177, 0.16327, 0.50603]</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.36036</td>\n",
       "      <td>[[14, 11, 11], [9, 4, 10], [9, 11, 21]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-11 21:37:54</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744677</td>\n",
       "      <td>0.762679</td>\n",
       "      <td>[0.18182, 0.34091, 0.41176]</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.3115</td>\n",
       "      <td>[0.15385, 0.45455, 0.34146]</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31662</td>\n",
       "      <td>[0.16667, 0.38961, 0.37333]</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.30987</td>\n",
       "      <td>[[4, 12, 10], [8, 15, 10], [10, 17, 14]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp epoch_trained_relative epochs_trained_total loss_train  \\\n",
       "0  2023-02-11 21:37:54                      0                    0   0.850509   \n",
       "1  2023-02-11 21:37:54                      1                    1   0.700464   \n",
       "2  2023-02-11 21:37:54                      2                    2   0.263107   \n",
       "3  2023-02-11 21:37:54                      3                    3   0.847592   \n",
       "4  2023-02-11 21:37:54                      4                    4   0.744677   \n",
       "\n",
       "  loss_valid              precision_class precision_avg_micro  \\\n",
       "0    0.59481  [0.30952, 0.29167, 0.11765]                0.24   \n",
       "1   0.798062  [0.38235, 0.40625, 0.20588]                0.33   \n",
       "2   0.825322    [0.45161, 0.48276, 0.225]                0.37   \n",
       "3   0.915717       [0.4375, 0.15385, 0.5]                0.39   \n",
       "4   0.762679  [0.18182, 0.34091, 0.41176]                0.33   \n",
       "\n",
       "  precision_avg_macro                 recall_class recall_avg_micro  \\\n",
       "0             0.23961  [0.33333, 0.17949, 0.18182]             0.24   \n",
       "1             0.33149  [0.30952, 0.38235, 0.29167]             0.33   \n",
       "2             0.38646      [0.41176, 0.4, 0.29032]             0.37   \n",
       "3             0.36378   [0.38889, 0.17391, 0.5122]             0.39   \n",
       "4              0.3115  [0.15385, 0.45455, 0.34146]             0.33   \n",
       "\n",
       "  recall_avg_macro                     f1_class f1_avg_micro f1_avg_macro  \\\n",
       "0          0.23155  [0.32098, 0.22223, 0.14286]         0.24      0.22869   \n",
       "1          0.32785   [0.3421, 0.39394, 0.24138]         0.33      0.32581   \n",
       "2          0.36736   [0.43077, 0.4375, 0.25352]         0.37      0.37393   \n",
       "3          0.35833  [0.41177, 0.16327, 0.50603]         0.39      0.36036   \n",
       "4          0.31662  [0.16667, 0.38961, 0.37333]         0.33      0.30987   \n",
       "\n",
       "                           confusion_matrix  \n",
       "0   [[13, 12, 14], [16, 7, 16], [13, 5, 4]]  \n",
       "1  [[13, 12, 17], [11, 13, 10], [10, 7, 7]]  \n",
       "2    [[14, 7, 13], [3, 14, 18], [14, 8, 9]]  \n",
       "3   [[14, 11, 11], [9, 4, 10], [9, 11, 21]]  \n",
       "4  [[4, 12, 10], [8, 15, 10], [10, 17, 14]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  tensor([0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]) \n",
      "  tensor([2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2])\n",
      "[[3 0 0]\n",
      " [0 1 2]\n",
      " [2 1 3]]\n",
      "{'precision_class': [0.6, 0.5, 0.6], 'precision_avg_micro': 0.58333, 'precision_avg_macro': 0.56667}\n",
      "{'recall_class': [1.0, 0.33333, 0.5], 'recall_avg_micro': 0.58333, 'recall_avg_macro': 0.61111}\n",
      "{'f1_class': [0.75, 0.4, 0.54545], 'f1_avg_micro': 0.58333, 'f1_avg_macro': 0.56515}\n"
     ]
    }
   ],
   "source": [
    "import dutils.type_check as _type_check\n",
    "import dutils.jupyter_ipython as _jupyter_ipython\n",
    "import torch\n",
    "import torch as _torch\n",
    "import numpy as _np\n",
    "from typing import List as _List\n",
    "import pandas as _pd\n",
    "from datetime import datetime as _datetime\n",
    "import ast as _ast\n",
    "\n",
    "\n",
    "class CategoricalMetrics:\n",
    "    \"\"\"\n",
    "    This is basically just a namespace with different functions designed to evaluate metrics e.g. accuracy.\n",
    "    INPUTS:\n",
    "    nc: int --> number of classes\n",
    "    preds: _torch.Tensor[int64 * nc] --> contain predictions. All values must be within [0, 1, ..., nc-1]\n",
    "    gt: _torch.Tensor[int64 * nc] --> contain ground truth labels. All values must be within [0, 1, ..., nc-1]\n",
    "    cfm: _np.ndarray[[int32 * nc] * nc] or None --> Confusion matrix between `gt` (rows) and `preds` (columns). If `cfm=None`, the confusion matrix will be calculated automatically\n",
    "    NOTE:\n",
    "    I would like to have all pytorch functionality in a single file, but would also like to be able to browse different metrcis with autocomplete.\n",
    "    Hence, why I made this into a class instead of ordinary functions / a seperate module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_decimal = 5\n",
    "\n",
    "    def _check_and_copy_input(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int, cfm: _np.ndarray = None):\n",
    "        # Type checks\n",
    "        _type_check.assert_types([preds, gt, nc, cfm], [_torch.Tensor, _torch.Tensor, int, _np.ndarray],\n",
    "                                 [False, False, False, True])\n",
    "        assert preds.dtype == _torch.int64, f\"Expected predictions to be af int64 (long), but recieved `preds.dtype={preds.dtype}`\"\n",
    "        assert gt.dtype == _torch.int64, f\"Expected ground truth labels to be af int64 (long), but recieved `preds.dtype={gt.dtype}`\"\n",
    "\n",
    "        # Value checks\n",
    "        assert nc > 1, f\"Expected at least 2 classes, but received `{nc}`\"\n",
    "        assert len(preds.shape) == 1, f\"Expected predictions to be of shape 'batch_size', but received `preds.shape={preds.shape}`\"\n",
    "        assert len(gt.shape) == 1, f\"Expected ground truth labels to be of shape 'batch_size', but received `gt.shape={gt.shape}`\"\n",
    "        assert gt.shape == preds.shape, \"Shape mismatch between the ground truth labels and the received predictions\"\n",
    "        assert (gt.max() < nc) and (gt.min() >= 0), \"At least one of the ground truth values are invalid\"\n",
    "        assert (preds.max() < nc) and (preds.min() >= 0), \"At least one prediction values are is invalid\"\n",
    "\n",
    "        # Confusion matrix\n",
    "        if cfm is not None:\n",
    "            assert cfm.shape == (nc, nc), f\"Expected the confusion matrix to be of shape `({nc, nc})`, but received `({cfm.shape})`\"\n",
    "            assert cfm.dtype == _np.int32, f\"Expected cfm to have dtype int32, but received `cfm.dtype={cfm.dtype}`\"\n",
    "            cfm = cfm.copy()\n",
    "\n",
    "        # Prepare tensors for metric calculations\n",
    "        preds = preds.clone().detach().cpu().float()\n",
    "        gt = gt.clone().detach().cpu().float()\n",
    "        return preds, gt, cfm\n",
    "\n",
    "    def acc(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int):\n",
    "        preds, gt, _ = self._check_and_copy_input(preds, gt, nc, None)\n",
    "        if nc != 2: raise NotImplementedError(\"Multiclass accuracy is not well defined, use recall instead.\")\n",
    "\n",
    "        acc = (preds == gt).float().mean().item()  # TODO: check implementation is correct\n",
    "        return acc\n",
    "\n",
    "    def precision(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int, cfm: _np.ndarray = None):\n",
    "        # Setup\n",
    "        if cfm is None: cfm = self.confusion_matrix(preds, gt, nc)\n",
    "        preds, gt, cfm = self._check_and_copy_input(preds, gt, nc, cfm)\n",
    "\n",
    "        # Precision calculation\n",
    "        TP = cfm.diagonal()\n",
    "        TP_plus_FP = cfm.sum(0)\n",
    "        precision_per_class = TP / (TP_plus_FP + 1e-12)\n",
    "        precision_per_class = precision_per_class.round(self.num_decimal)\n",
    "\n",
    "        return {\"precision_class\": precision_per_class.tolist(),\n",
    "                \"precision_avg_micro\": round(TP.sum() / TP_plus_FP.sum(), self.num_decimal),\n",
    "                \"precision_avg_macro\": round(precision_per_class.mean(), self.num_decimal)}\n",
    "\n",
    "    def recall(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int, cfm: _np.ndarray = None):\n",
    "        # Setup\n",
    "        if cfm is None: cfm = self.confusion_matrix(preds, gt, nc)\n",
    "        preds, gt, cfm = self._check_and_copy_input(preds, gt, nc, cfm)\n",
    "\n",
    "        # Recall calculation\n",
    "        TP = cfm.diagonal()\n",
    "        TP_plus_FN = cfm.sum(1)\n",
    "        recall_per_class = TP / (TP_plus_FN + 1e-12)\n",
    "        recall_per_class = recall_per_class.round(self.num_decimal)\n",
    "\n",
    "        return {\"recall_class\": recall_per_class.tolist(),\n",
    "                \"recall_avg_micro\": round(TP.sum() / TP_plus_FN.sum(), self.num_decimal),\n",
    "                \"recall_avg_macro\": round(recall_per_class.mean(), self.num_decimal)}\n",
    "\n",
    "    def f1_score(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int, cfm: _np.ndarray = None):\n",
    "        # Setup\n",
    "        if cfm is None: cfm = self.confusion_matrix(preds, gt, nc)\n",
    "        preds, gt, cfm = self._check_and_copy_input(preds, gt, nc, cfm)\n",
    "\n",
    "        # F1 score calculation (macro)\n",
    "        precision = _np.array(self.precision(preds.long(), gt.long(), nc, cfm)[\"precision_class\"])\n",
    "        recall = _np.array(self.recall(preds.long(), gt.long(), nc, cfm)[\"recall_class\"])\n",
    "        f1_per_class = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "        f1_per_class = f1_per_class.round(self.num_decimal)\n",
    "\n",
    "        # F1 score calculation (micro)\n",
    "        # So to make a long story short \"f1=precision=recall\" is true in multiclass setups with micro averging.\n",
    "        # The reason for this is essentailly that FP=FN ==> precision = recall.\n",
    "        # This was kinda wierd to me at first, but in a multiclass setup all the elements in a conf. matrix is both FP and FN simulationously i.e. a wrong prediction in one class will always be missing in another\n",
    "        # So I have just calculated the precision ones to avoid unnecessary computations\n",
    "        TP = cfm.diagonal().sum()\n",
    "        FP = cfm.sum(0).sum() - TP  # == cfm.sum(1).sum() - TP\n",
    "        f1_avg_micro = round((TP / (TP + FP)).mean(), self.num_decimal)\n",
    "\n",
    "        return {\"f1_class\": f1_per_class.tolist(),\n",
    "                \"f1_avg_micro\": round(f1_avg_micro, self.num_decimal),\n",
    "                \"f1_avg_macro\": round(f1_per_class.mean(), self.num_decimal)}\n",
    "\n",
    "    def class_balance(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int, label_names: _List[str],\n",
    "                      plot_class_dist: bool = False):\n",
    "        # _np.unique(labels.numpy(), return_counts=True)\n",
    "        # plot_class_dist\n",
    "        # translate numbers to names\n",
    "        raise NotImplementedError(\"\")\n",
    "\n",
    "    def confusion_matrix(self, preds: _torch.Tensor, gt: _torch.Tensor, nc: int):\n",
    "        preds, gt, _ = self._check_and_copy_input(preds, gt, nc, None)\n",
    "        cfm = _np.zeros((nc, nc))\n",
    "        for p, l in zip(preds.long(), gt.long()):\n",
    "            cfm[l, p] += 1\n",
    "        return cfm.astype(int)\n",
    "\n",
    "\n",
    "pytorch_metrics = CategoricalMetrics()\n",
    "\n",
    "\n",
    "class CategoricalLogger:\n",
    "    \"\"\"\n",
    "    # EXAMPLE (1)\n",
    "    >> logger = CategoricalLogger(1000, 3)\n",
    "    >> for epoch in range(5):\n",
    "    >>    logger.update(epoch, _torch.randint(0, 3, (10000,)).long(), _torch.randint(0, 3, (10000,)).long())\n",
    "    >> print(logger)\n",
    "    >> logger.get_overall_average()\n",
    "    \n",
    "    \n",
    "    # EXAMPLE (2)\n",
    "    >> logger = CategoricalLogger(10, 3)\n",
    "    >> for epoch in range(5):\n",
    "    >>     logger.update(epoch, _torch.randint(0, 3, (100,)).long(), _torch.randint(0, 3, (100,)).long(),\n",
    "    >>                   _torch.rand(1)[0], _torch.rand(1)[0])\n",
    "    >> print(logger)\n",
    "    >> logger.get_overall_average()\n",
    "    >> predictions = _torch.tensor([0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]).long()\n",
    "    >> labels = _torch.tensor([2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]).long()\n",
    "    >> print(\" \", predictions, \"\\n \", labels)\n",
    "    >> C = CategoricalMetrics()\n",
    "    >> print(C.confusion_matrix(predictions, labels, 3))\n",
    "    >> print(C.precision(predictions, labels, 3))\n",
    "    >> print(C.recall(predictions, labels, 3))\n",
    "    >> print(C.f1_score(predictions, labels, 3))\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, batch_size: int, num_classes: int, epochs_trained_prior: int = 0, acc: bool = False,\n",
    "                 precision: bool = True, recall: bool = True, f1: bool = True, confusion_matrix: bool = True):\n",
    "        if acc and not (num_classes != 2):\n",
    "            raise ValueError(\"Accuracy is only defined for binary classification tasks\")\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs_trained_prior = epochs_trained_prior\n",
    "\n",
    "        # Prepare metrics\n",
    "        extra_cols = []\n",
    "        self.metrics = []\n",
    "        if acc:\n",
    "            extra_cols += [\"acc\"]\n",
    "            self.metrics.append(\"acc\")\n",
    "        if precision:\n",
    "            extra_cols += ['precision_class', 'precision_avg_micro', 'precision_avg_macro']\n",
    "            self.metrics.append(\"precision\")\n",
    "        if recall:\n",
    "            extra_cols += ['recall_class', 'recall_avg_micro', 'recall_avg_macro']\n",
    "            self.metrics.append(\"recall\")\n",
    "        if f1:\n",
    "            extra_cols += ['f1_class', 'f1_avg_micro', 'f1_avg_macro']\n",
    "            self.metrics.append(\"f1\")\n",
    "        if confusion_matrix:\n",
    "            extra_cols += [\"confusion_matrix\"]\n",
    "            self.metrics.append(\"confusion_matrix\")\n",
    "\n",
    "        df_columns = [\"timestamp\", \"epoch_trained_relative\", \"epochs_trained_total\", \"loss_train\", \"loss_valid\"]\n",
    "        self.df = _pd.DataFrame(columns=df_columns)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        if _jupyter_ipython.in_jupyter():\n",
    "            display(self.df)  # This will just display the pandas dataframe as normally in jupyter notebook\n",
    "            return \"\"\n",
    "        else:\n",
    "            return str(self.df)\n",
    "\n",
    "    def _calculate_metric(self, metric, preds, gt):\n",
    "        cfm = pytorch_metrics.confusion_matrix(preds, gt, self.num_classes)\n",
    "        if metric == \"acc\":\n",
    "            return pytorch_metrics.acc(preds, gt, self.num_classes)\n",
    "        if metric == \"precision\":\n",
    "            return pytorch_metrics.precision(preds, gt, self.num_classes, cfm=cfm)\n",
    "        if metric == \"recall\":\n",
    "            return pytorch_metrics.recall(preds, gt, self.num_classes, cfm=cfm)\n",
    "        if metric == \"f1\":\n",
    "            return pytorch_metrics.f1_score(preds, gt, self.num_classes, cfm=cfm)\n",
    "        if metric == \"confusion_matrix\":\n",
    "            return cfm\n",
    "\n",
    "    def update(self, current_epoch, preds: _torch.Tensor, gt: _torch.Tensor, loss_train:_torch.Tensor=None, loss_valid:_torch.Tensor=None):\n",
    "        # Checks\n",
    "        _type_check.assert_types([current_epoch, preds, gt, loss_train, loss_valid], [int] + [_torch.Tensor]*4, [False, False, False, True, True])\n",
    "\n",
    "        # Add new row which will be populated one variable at a time\n",
    "        i = len(self.df)\n",
    "        self.df.loc[i] = None\n",
    "\n",
    "        # Simple logging stuff\n",
    "        self.df.loc[i, \"epoch_trained_relative\"] = current_epoch\n",
    "        self.df.loc[i, \"epochs_trained_total\"] = self.epochs_trained_prior + current_epoch\n",
    "        self.df.loc[i, \"timestamp\"] = _datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Losses\n",
    "        if loss_train:\n",
    "            self.df.loc[i, \"loss_train\"] = loss_train.clone().cpu().detach().item()\n",
    "            assert loss_valid.shape == _torch.Size([]), f\"Expected `loss_valid` be a single number, but received `loss_train.shape={loss_valid.shape}`\"\n",
    "        if loss_valid:\n",
    "            self.df.loc[i, \"loss_valid\"] = loss_valid.clone().cpu().detach().item()\n",
    "            assert loss_train.shape == _torch.Size([]), f\"Expected `train_loss` be a single number, but received `loss_train.shape={loss_train.shape}`\"\n",
    "\n",
    "        # Calculate all the metrics and add them one at a time.\n",
    "        # Some metrics return more then one value (per class, avg_micro, ...). This is handled with dicts\n",
    "        for metric in self.metrics:\n",
    "            return_value = self._calculate_metric(metric, preds, gt)\n",
    "            if isinstance(return_value, dict):\n",
    "                for name, value in return_value.items():\n",
    "                    self.df.loc[i, name] = str(value)\n",
    "            elif metric == \"confusion_matrix\":\n",
    "                self.df.loc[i, metric] = str(return_value.tolist())\n",
    "            else:\n",
    "                self.df.loc[i, metric] = str(return_value)\n",
    "\n",
    "        # Check if there's any illegal NAs (only train_loss and valid_loss is allowed to be NA, hence the drop)\n",
    "        assert not any(self.df.drop(columns=[\"loss_train\", \"loss_valid\"]).iloc[i].isna().tolist()), \\\n",
    "            f\"At least one value was determined to be NA. The problem occurred in row: {self.df.loc[i]}\"\n",
    "\n",
    "    def get_overall_average(self):\n",
    "        df_combined = self.df.iloc[0:0].copy()\n",
    "        df_combined = df_combined.drop(columns=[\"timestamp\"])\n",
    "        df_combined.loc[0] = None\n",
    "\n",
    "        for col_name in df_combined:\n",
    "            values_combined = _np.array([_ast.literal_eval(str(l)) for l in self.df[col_name].tolist() if str(l) != \"nan\"])\n",
    "            if len(values_combined) == 0:\n",
    "                if col_name not in [\"loss_train\", \"loss_valid\"]: raise RuntimeError(\"Unexpected error. Probably caused by illegal NAs\")\n",
    "                df_combined[col_name] = None\n",
    "            elif col_name in [\"epoch_trained_relative\", \"epochs_trained_total\"]:\n",
    "                df_combined[col_name] = self.df[col_name].max()\n",
    "            elif \"class\" in col_name:\n",
    "                df_combined[col_name] = str(values_combined.mean(0))\n",
    "            elif col_name == \"confusion_matrix\":\n",
    "                df_combined[col_name] = str(values_combined.sum(0).tolist())\n",
    "            else:\n",
    "                df_combined[col_name] = values_combined.mean()\n",
    "        return df_combined\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger = CategoricalLogger(10, 3)\n",
    "    for epoch in range(5):\n",
    "        logger.update(epoch, _torch.randint(0, 3, (100,)).long(), _torch.randint(0, 3, (100,)).long(),\n",
    "                      _torch.rand(1)[0], _torch.rand(1)[0])\n",
    "    print(logger)\n",
    "    logger.get_overall_average()\n",
    "\n",
    "\n",
    "    predictions = _torch.tensor([0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]).long()\n",
    "    labels = _torch.tensor([2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]).long()\n",
    "\n",
    "    print(\" \", predictions, \"\\n \", labels)\n",
    "    C = CategoricalMetrics()\n",
    "    print(C.confusion_matrix(predictions, labels, 3))\n",
    "    print(C.precision(predictions, labels, 3))\n",
    "    print(C.recall(predictions, labels, 3))\n",
    "    print(C.f1_score(predictions, labels, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
